Simulation Exercise - Omitted Variable Bias
Anthony Restivo
10/07/2021

Introduction
The purpose of this exercise is to understand how omitted variable bias can impact your estimates and lead to inferential errors.

On your path to understanding this, you are going to “play god” in some sense. You are going to construct a population with known parameters. You will then sample from this population, and estimate a misspecified model.

The misspecification in this particular model will be that it omits relevant independent variables that are correlated with the independent variables included in our model. This would be a violation of the “zero conditional mean assumption”, also known as the exogeneity assumption.

In doing this process, you will learn how this common type of misspecification can lead you to draw the wrong conclusions regarding relationships between variables.

Simulate data
Q0
Run all and knit!

Q1
Generate 300 observations of a disturbance 
 from the standard normal distribution.

Reminder- the standard normal distribution is the Normal distribution with mean=0 and standard deviation =1.

u <- rnorm(300,0,1)
Plot the disturbances in a histogram.

qplot(u)
## Warning: `qplot()` was deprecated in ggplot2 3.4.0.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.


Q2
Generate 300 observations of another disturbance 
 from the standard normal distribution

e <- rnorm(300,0,1)
Q3
Generate 300 observations of an explanatory variable 
 from the standard normal distribution.

x1 <- rnorm(300,0,1)
Q4
Construct a second explanatory variable, 
 using your variables generated in steps 2-3 such that it is related to 
 in the following way:


x2 <- x1 +e
Q5
Construct a scatter plot to visualize the relationship between x1 and x2.

qplot(x2,x1)
 What correlation do you observe between 
 and 
?

A:Both x2 and x1 exhibit a horizontal increase

Q6
Using your simulated variables, construct the dependent variable 
 according to the following:

 y <- 1 + x1 -3*x2+u
Note that you have just constructed the population model.

Remind yourself that, in practice, you would never actually observe the population model, but here you get to have the fun of being an all-powerful deity in a toy universe.

The idea is that, by constructing the model, we know exactly what the parameters are, and will be able to learn about how a particular type of violation of our assumptions will end up impacting our estimates.

Q7
Based on what have you simulated so far, consider the following:

Q:What is the “true” partial effect of x1 on y?

A:The true partial effect is the coefficient within the whole plot, identifying the shifts in y.

Q: What is the “true” partial effect of x2 on y?

A:The true partial effect of x2 on y is the sum of all the coefficients showing the major shifts in y.

Q: Outside of the this lab simulation, would we ever be able to observe these “true” population parameters?

A:

Data Analysis
Q8
Now, suppose that in a rush you estimate the follow misspecified model:
where 
 is the dependent variable constructed according to the model Q6, and 
 is the independent variable constructed in Q3.

Q: What does r represent in this model?

A:Any unobserved determinant of y

Q: Why would we say this model is misspecified?

A:We can agree x2 sitting inside of r. We should be able to agree that x1 and x2 are related because we built them to be related. We left out a relevant variable.

Q: Make a scatter plot with 
 on the horizontal axis and 
 on the vertical axis. In this naive analysis, what relationship do you observe between 
 and 
?

qplot(x1,y)
 A: A downward slope relationship

Q9
Estimate this model using 3 different “random” samples: 1) observations 1-100 2) observations 101-200 3) observations 201-300

m1 <- lm(y[1:100]~x1[1:100])
m2 <- lm(y[101:200]~x1[101:200])
m3 <- lm(y[201:300]~x1[201:300])

summary(m1)
## 
## Call:
## lm(formula = y[1:100] ~ x1[1:100])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.0126 -2.3382 -0.0351  2.0793  9.9994 
## 
## Coefficients:
##             Estimate Std. Error t value    Pr(>|t|)    
## (Intercept)   1.0388     0.3187   3.260     0.00153 ** 
## x1[1:100]    -1.5756     0.2913  -5.408 0.000000449 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.133 on 98 degrees of freedom
## Multiple R-squared:  0.2299, Adjusted R-squared:  0.222 
## F-statistic: 29.25 on 1 and 98 DF,  p-value: 0.0000004495
summary(m2)
## 
## Call:
## lm(formula = y[101:200] ~ x1[101:200])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -12.7768  -2.1490   0.5352   1.9338   7.8387 
## 
## Coefficients:
##             Estimate Std. Error t value       Pr(>|t|)    
## (Intercept)   1.0797     0.3516   3.071        0.00276 ** 
## x1[101:200]  -2.2637     0.3220  -7.030 0.000000000279 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.516 on 98 degrees of freedom
## Multiple R-squared:  0.3352, Adjusted R-squared:  0.3284 
## F-statistic: 49.41 on 1 and 98 DF,  p-value: 0.0000000002792
summary(m3)
## 
## Call:
## lm(formula = y[201:300] ~ x1[201:300])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.8320 -2.4454 -0.1866  2.2336  8.2227 
## 
## Coefficients:
##             Estimate Std. Error t value          Pr(>|t|)    
## (Intercept)   1.1673     0.3102   3.763          0.000286 ***
## x1[201:300]  -2.4066     0.2865  -8.401 0.000000000000351 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.102 on 98 degrees of freedom
## Multiple R-squared:  0.4187, Adjusted R-squared:  0.4127 
## F-statistic: 70.58 on 1 and 98 DF,  p-value: 0.0000000000003511
Q10
Interpret the slope estimate from the first sample.

A:-1.5264

Q11
Why are your estimates slightly different across samples?

A:The samples are slightly different across due to the fact of the out of the blue differences within the population.

Q12
Compare the slope estimates from your 3 sample to the “true” population parameters?

How far are these estimates from the true partial effect of 
 on 
?

A:Not far at all from a true partial effect because their is regression containing a major negative connection.

Q13
Now, run an appropriately specified model. Regression y on x1 and x2, using the 3 different samples outlined in Q9 above.

ols1 <- lm(y[1:100]~x1[1:100]+x2[1:100])
ols2 <- lm(y[101:200]~x1[101:200]+x2[101:200])
ols3 <- lm(y[201:300]~x1[201:300]+x2[201:300])

summary(ols1)
## 
## Call:
## lm(formula = y[1:100] ~ x1[1:100] + x2[1:100])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.3003 -0.5898 -0.0582  0.5824  2.1611 
## 
## Coefficients:
##             Estimate Std. Error t value             Pr(>|t|)    
## (Intercept)  1.12422    0.09191  12.232 < 0.0000000000000002 ***
## x1[1:100]    1.10370    0.11701   9.433  0.00000000000000225 ***
## x2[1:100]   -3.03627    0.09231 -32.892 < 0.0000000000000002 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.9033 on 97 degrees of freedom
## Multiple R-squared:  0.9366, Adjusted R-squared:  0.9353 
## F-statistic: 716.9 on 2 and 97 DF,  p-value: < 0.00000000000000022
summary(ols2)
## 
## Call:
## lm(formula = y[101:200] ~ x1[101:200] + x2[101:200])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7285 -0.6076 -0.1670  0.5415  2.4441 
## 
## Coefficients:
##             Estimate Std. Error t value             Pr(>|t|)    
## (Intercept)  0.96634    0.09579  10.088 < 0.0000000000000002 ***
## x1[101:200]  0.94101    0.12679   7.422      0.0000000000445 ***
## x2[101:200] -2.89375    0.08269 -34.995 < 0.0000000000000002 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.9574 on 97 degrees of freedom
## Multiple R-squared:  0.9512, Adjusted R-squared:  0.9502 
## F-statistic: 945.5 on 2 and 97 DF,  p-value: < 0.00000000000000022
summary(ols3)
## 
## Call:
## lm(formula = y[201:300] ~ x1[201:300] + x2[201:300])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.87498 -0.50594 -0.00288  0.66185  2.60310 
## 
## Coefficients:
##             Estimate Std. Error t value             Pr(>|t|)    
## (Intercept)   1.0276     0.1045   9.831  0.00000000000000031 ***
## x1[201:300]   1.0528     0.1577   6.676  0.00000000153259635 ***
## x2[201:300]  -3.0460     0.1099 -27.720 < 0.0000000000000002 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.044 on 97 degrees of freedom
## Multiple R-squared:  0.9348, Adjusted R-squared:  0.9335 
## F-statistic: 695.8 on 2 and 97 DF,  p-value: < 0.00000000000000022
Q14
How do your estimates in the appropriately specified model compare to the true population parameters?

A:The value of the estimates have no influence , meeting the values that makes sense.

Q15
Consider what econometric theory has to say about the expected value of the slope estimate in a model with omitted variables (equation 3.45 in chapter 3).
 
 
  
 
 
  
 
 
 
where 
 and 
 are the “true” population parameters, and 
 is the slope from a regression of 
 on 
.

The “problem” is that we do not observe the two pieces of information on the right hand side - we just observe the single piece of information, the slope estimate from the misspecified model 
, and know that it is a combination of the true parameter 
 and the bias term 
 .

Hence, our estimate of 
 is biased in our misspecified model that has omitted variables - its expected value is not the true population parameter.

We will now try to “verify” that the relationship above from theory holds by reconstructing it.

Q16
Run the regression of 
 on 
 to estimate 
.

aux <- lm(x2[1:100]~x1[1:100])
summary(aux)
## 
## Call:
## lm(formula = x2[1:100] ~ x1[1:100])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.58156 -0.55777  0.00381  0.75314  2.00530 
## 
## Coefficients:
##             Estimate Std. Error t value             Pr(>|t|)    
## (Intercept)  0.02814    0.10054    0.28                 0.78    
## x1[1:100]    0.88244    0.09192    9.60 0.000000000000000892 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.9885 on 98 degrees of freedom
## Multiple R-squared:  0.4847, Adjusted R-squared:  0.4794 
## F-statistic: 92.17 on 1 and 98 DF,  p-value: 0.0000000000000008924
$ =.9

Q17
Plug in your estimate of 
 along with the true parameter values 
 and 
 to get the slope estimate in the misspecified model as predicted by econometric theory.


Q18
How does the slope in the misspecified model, as predicted by theory above in Q17 compare to the actual slope estimate from the misspecified models in Q9?

That is - plug in values for the right hand side of the equation in Q17, and compare your result to your estimates of 
 in from the models in Q9

A:The slope in q9 is progressively rising while this slope is curling downward decreasing.

Q20
What is the estimate from the misspecified model capturing that introduces this bias?

A:The whole idea is when you regress y only on x1 the effect of x1 is going to partially effect the effect of x2 and x1 ends up decreasing y due to x1 and x1 being related.

Q21
Some bigwig at your company asks you about the impact of X1 on Y. Explain how you could draw drastically different conclusions between your estimates from Q9 and those obtained in Q13. What do your results suggest about this particular type of model misspecification?

A: We need to show them how we can get different estimates between doing things the right way with specified model and not doing the incorrect way the mi specified model.

Q22
Chunk below reserved for demonstration at end of video!

results <- data.frame(x1coef=NA)
  for (i in 1:500){ 
    error1 <- rnorm(500)
    error2 <- rnorm(500)
    x1 <- rnorm(500)
    x2 <-  x1+error1
    y=1+x1-3*x2+error2
    olsmodel <- lm(y~x1)
    results[i,] <- coef(olsmodel)[['x1']]
}
hist(as.numeric(results$x1coef))

results <- data.frame(x1coef=NA)
  for (i in 1:500){ 
    error1 <- rnorm(500)
    error2 <- rnorm(500)
    x1 <- rnorm(500)
    x2 <-  x1+error1
    y=1+x1-3*x2+error2
    olsmodel <- lm(y~x1+x2) 
    results[i,] <- coef(olsmodel)[[ x1']]
}
hist(as.numeric(results$x1coef))
## Error in parse(text = input): <text>:21:39: unexpected INCOMPLETE_STRING
## 22: }
## 23: hist(as.numeric(results$x1coef))
##                                           ^
